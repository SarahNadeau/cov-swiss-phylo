---
title: "Plot introductions vs. European incidence"
author: nadeaus
date: 12.10.21
output: html_notebook
---

This script is to summarize introductions and their persistence, then compare them to null models in the absence of interventions.

```{r, include=FALSE}
getwd()
require(dplyr)
require(tidyr)
require(ggplot2)
require(cowplot)
require(zoo)
library(lubridate)
library(tidybayes)

source("../sars_cov_2/grapevine/utility_functions.R")
source("../sars_cov_2/grapevine/generate_figures/functions.R")
source("scripts/figures_shared_vars.R")
```

Load grapevine data

```{r}
workdir <- "results_all/2021-08-10_for_manuscript_rep_1"
outdir <- paste(workdir, "output", sep = "/")
outdir <- "figures"
grapevine_results <- load_grapevine_results(
  workdir = workdir,
  min_chain_size = 1,
  viollier_only = F
)

# Load day to week mapping
day_to_week <- read.csv("results_all/bdsky/log_files/SwissTransmissionChains/sequences/date_to_week.csv") %>%
  full_join(data.frame(date = "2020-12-01", week = "2020-11-30")) %>% # add missing week for last sample date
  mutate(date = as.Date(date), week = as.Date(week))

# Annotate introductions with summary information
chain_summary_stats <- grapevine_results$samples %>%
  group_by(tree, chains_assumption, chain_idx) %>%
  summarize(
    first_sample_date = min(date),
    last_sample_date = max(date),
    .groups = "drop"
  ) %>%
  mutate(time_to_last_sample = as.integer(difftime(last_sample_date, first_sample_date, units = "days"))) %>%
  full_join(y = grapevine_results$chains, by = c("tree", "chains_assumption", "chain_idx"))
```

Summarize early introduction inferences

```{r}
early_introductions <- grapevine_results$samples %>%
  left_join(chain_summary_stats) %>%
  filter(first_sample_date < as.Date("2020-03-10")) %>%
  mutate(
    chain_idx_2 = dplyr::group_indices(., chains_assumption, chain_idx),
    starts_before_1_mar = first_sample_date < as.Date("2020-03-01")
  )

ggplot(data = early_introductions, aes(x = date, y = chain_idx_2, color = size == 1)) +
  facet_grid(chains_assumption ~ ., scales = "free_y", labeller = "label_both") +
  geom_vline(xintercept = as.Date("2020-02-25"), linetype = "dashed") + 
  geom_point() +
  theme(legend.position = "bottom")

ggsave("figures/early_introductions.png", width = single_col_width, height = single_col_width, units = "cm")
```

Load case data for European countries

```{r}
CASE_COUNT_LINK <- "https://opendata.ecdc.europa.eu/covid19/casedistribution/csv/"
cases_raw <- read.csv(url(CASE_COUNT_LINK), na.strings = "", fileEncoding = "UTF-8-BOM") %>%
  filter(continentExp == "Europe") %>%
  select(countryterritoryCode, dateRep, cases) %>%
  mutate(date = as.Date(dateRep, "%d/%m/%Y")) %>%
  filter(date <= as.Date("2020-12-01"))

cases_summary <- cases_raw %>%
  mutate(country = case_when(
    countryterritoryCode == "CHE" ~ "Switzerland",
    countryterritoryCode == "FRA" ~ "France",
    countryterritoryCode == "DEU" ~ "Germany",
    countryterritoryCode == "ITA" ~ "Italy",
    countryterritoryCode == "AUT" ~ "Austria",
    T ~ "Other European"
  )) %>%
  group_by(date, country) %>%
  summarize(cases = sum(cases), .groups = "drop") %>%
  group_by(country) %>%
  arrange(date) %>%
  mutate(weekly_avg_new_cases = zoo::rollmean(cases, 7, align = "center", na.pad = T))
```

Get mean persistence across whole period
```{r}
chain_summary_stats %>%
  group_by(chains_assumption) %>%
  summarize(
    mean_persistence = mean(time_to_last_sample),
    sd_persistence = sd(time_to_last_sample),
    median_persistence = median(time_to_last_sample)
  )
```

Summarize introductions by first sample date, their persistence

```{r}
# Get number of introductions first sampled each week
introduction_summary <- chain_summary_stats %>%
  mutate(date = as.Date(first_sample_date)) %>%
  left_join(day_to_week) %>%
  group_by(week, chains_assumption) %>%
  summarise(introductions_by_first_sample_date = n(), .groups = "drop") %>%
  mutate(date = week, country = "Switzerland")

persistence_summary <- chain_summary_stats %>% # monthly summary
  mutate(date = as.Date(format(first_sample_date, "%Y-%m-01"))) %>%
  group_by(date, chains_assumption) %>%
  summarise(
    frac_60_day_persisters = sum(time_to_last_sample >= 60) / n()
  ) %>%
  mutate(country = "Switzerland")

# exclude undefined values for persistence
max_date <- max(grapevine_results$samples$date) - 60
persistence_summary[persistence_summary$date > max_date, "frac_60_day_persisters"] <- NA

highlight_data <- data.frame(
  date_start = as.Date(c("2020-03-13", "2020-03-17")),
  date_end = as.Date(c("2020-06-15", "2020-04-27")),
  event = c("Borders closed", "Partial lockdown")
)

measures_fill_scale <- scale_fill_manual(values = c(
  "Borders closed" = "black",
  "Partial lockdown" = "grey"
))

introductions_through_time <- ggplot(
  data = introduction_summary,
  aes(x = date, y = introductions_by_first_sample_date)
) +
  geom_point(aes(color = chains_assumption, shape = chains_assumption)) + 
  geom_line(aes(color = chains_assumption), linetype = "solid") + 
  scale_x_date(date_breaks = "1 month", date_labels = "%b.", limits = as.Date(c("2020-01-28", "2020-12-03"))) +
  ylim(0, NA) +
  xlab(NULL) +
  ylab("Newly sampled\nintroductions per week") +
  shared_theme +
  scale_color_manual(
    labels = chain_assumption_labs, 
    values = chains_assumption_colors
  ) +
  scale_shape(labels = chain_assumption_labs) +
  measures_fill_scale +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_rect(
    inherit.aes = F, data = highlight_data %>% filter(event == "Borders closed"),
    aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = Inf, fill = event), alpha = 0.3
  )

persistence_through_time <- ggplot(
  data = persistence_summary  %>% filter(date < as.Date("2020-10-01")),  # don't plot within 60 days of end sampling period
  aes(x = date, y = frac_60_day_persisters)
) +
  geom_point(aes(color = chains_assumption, shape = chains_assumption)) + 
  geom_line(aes(color = chains_assumption), linetype = "solid") + 
  scale_x_date(date_breaks = "1 month", date_labels = "%b.", limits = as.Date(c("2020-01-28", "2020-12-03"))) +
  ylim(0, NA) +
  xlab(NULL) +
  ylab("Fraction of newly\nsampled introductions\npersisting at least 60 days") +
  shared_theme +
  scale_color_manual(
    labels = chain_assumption_labs, 
    values = chains_assumption_colors
  ) +
  scale_shape(labels = chain_assumption_labs) +
  measures_fill_scale +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_rect(
    inherit.aes = F, data = highlight_data %>% filter(event == "Partial lockdown"),
    aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = Inf, fill = event), alpha = 0.3
  )

introductions_through_time
persistence_through_time
```

Calculate probability an introduction starting each week goes unsampled until end of sampling period
Note: probabilities calculated from first day of each week
See Kuehnert et al. 2013 in PNAS
Implementation adapted from function "preCalculation" in https://github.com/BEAST2-Dev/bdsky/blob/master/src/beast/evolution/speciation/BirthDeathSkylineModel.java

```{r}
# Load median Re estimates from BDSKY analysis
re_by_week_min <- read.csv(file = "results_all/bdsky/log_files/SwissTransmissionChains/re_by_interval_start.csv") %>%
  filter(clusters == "min") %>%
  arrange(date)
re_by_week_max <- read.csv(file = "results_all/bdsky/log_files/SwissTransmissionChains/re_by_interval_start.csv") %>%
  filter(clusters == "max") %>%
  arrange(date)

# Get time (in years) from start of process to start of each interval
breakpoint_days <- c(
  sort(as.Date(unique(re_by_week_min$date)[!is.na(unique(re_by_week_min$date))])),
  as.Date("2020-12-01") # end of sampling period = end of process
)
times <- as.numeric(difftime(breakpoint_days, as.Date("2019-12-30"), units = "days") / 365.25)
m <- length(times)

# Assume constant sampling proportion, constant death rate, no single-timepoint sampling efforts
s <- 0.05
delta <- 36.5
rho <- 0

# Get extinction probabilities for each set of Re estimates (conditioned on min/max introductions)
is_first <- T
for (idx in 1:2) {
  if (idx == 1) {
    re_by_week <- re_by_week_min
  } else {
    re_by_week <- re_by_week_max
  }
  for (est_type in c("median", "low", "high")) {
    # Calculate birth, death, sampling rates from epi parameterization
    re_estimates <- re_by_week[[est_type]]
    lambda <- re_estimates * delta
    mu <- delta - s * delta
    psi <- s * delta

    # Initialize values for extinction probability calculation
    Ai <- rep(0, m)
    Bi <- rep(0, m)
    p <- rep(0, m)

    # Calculate Ai
    for (i in 1:m) {
      Ai[i] <- sqrt((lambda[i] - mu - psi)^2 + 4 * lambda[i] * psi)
    }

    calc_Bi <- function(lambda, mu, psi, rho, A, p) {
      return(((1 - 2 * p * (1 - rho)) * lambda + mu + psi) / A)
    }

    calc_p <- function(lambda, mu, psi, A, B, t, ti) {
      p <- (lambda + mu + psi - A * (exp(A * (t - ti)) * (1 + B) - (1 - B)) / (exp(A * (t - ti)) * (1 + B) + (1 - B))) / (2 * lambda)
      return(p)
    }

    # Calculate last Bi (where p_m+1(t_m) = 1)
    Bi[m - 1] <- calc_Bi(lambda[m - 1], mu, psi, rho, Ai[m - 1], 1)
    p[m] <- 1

    # Calculate Bi and p moving from last to first time interval
    for (i in rev(1:(m - 1))) {
      p[i] <- calc_p(lambda[i], mu, psi, Ai[i], Bi[i], times[i + 1], times[i])
      if (i > 1) {
        Bi[i - 1] <- calc_Bi(lambda[i - 1], mu, psi, rho, Ai[i - 1], p[i])
      }
    }

    # Summarize results
    print(paste("Idx:", idx, "est_type:", est_type))
    p_results_tmp <- data.frame(
      week = breakpoint_days,
      prob_being_sampled = 1 - p,
      chains_assumption = case_when(idx == 1 ~ "min", T ~ "max"),
      estimate_type = est_type
    ) %>% left_join(
      re_by_week %>% 
        mutate(week = as.Date(date), re_median = median, re_low = low, re_high = high) %>% 
        select(week, re_median, re_low, re_high)
    )

    if (is_first) {
      is_first <- F
      p_results <- p_results_tmp
    } else {
      p_results <- rbind(p_results, p_results_tmp)
    }
  }
}

p_results_wide <- tidyr::pivot_wider(
  p_results,
  id_cols = c(week, chains_assumption, re_high, re_low, re_median),
  id_expand = F,
  names_from = "estimate_type",
  values_from = "prob_being_sampled",
  names_prefix = "prob_being_sampled_"
)

# Plot probability of an introduction being sampled over time
ggplot(data = p_results_wide, aes(x = week)) +
  geom_lineribbon(
    aes(y = prob_being_sampled_median, ymax = prob_being_sampled_high, ymin = prob_being_sampled_low, fill = "Probability of introduction being sampled"), 
    alpha = 0.3
  ) +
  geom_lineribbon(
    aes(y = re_median, ymax = re_high, ymin = re_low, fill = "Re estimate"), 
    alpha = 0.3
  ) +
  facet_grid(chains_assumption ~ ., labeller = labeller(chains_assumption = chain_assumption_labs)) +
  theme_bw() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  labs(x = element_blank(), y = "Value")
ggsave(file = "figures/prob_introduction_being_sampled_2.png", width = single_col_width, height = single_col_width, units = "cm")
```

Incorporate this in model until border closure date: introductions (lagged from first sampling) ~ europe incidence

```{r}
# Test different lags from European incidence to sampled introduction,
# up to 18 days based on 10-day infectious period, LdP's UK estimate of 10-18 day introduction to genome sample delay
case_data_base <- cases_raw %>%
  filter(
    countryterritoryCode != "CHE",
    # ) %>%  # for all Europe
    countryterritoryCode %in% c("ITA", "FRA", "DEU", "AUT")
  ) %>% # for only neighboring countries
  group_by(date) %>%
  summarize(cases = sum(cases), .groups = "drop") %>%
  arrange(date) %>%
  mutate(weekly_avg_new_cases = zoo::rollmean(cases, 7, align = "center", na.pad = T)) %>%
  select(-cases)

stop_fit_date <- as.Date("2020-03-13")
close_date <- as.Date("2020-03-13")
re_open_date <- as.Date("2020-06-15")
end_plot_date <- as.Date("2020-06-15")
end_summer_holiday_date <- as.Date("2020-08-28")
plausible_delays <- seq(from = 1, to = 18, by = 1)
is_first <- T
for (delay in plausible_delays) {

  # Generate model data: introductions are lagged delay days before sampling
  model_data_tmp <- chain_summary_stats %>%
    mutate(date = as.Date(first_sample_date) - delay) %>%
    left_join(day_to_week) %>%
    group_by(week, chains_assumption) %>%
    summarise(introductions_by_week_raw = n(), .groups = "drop") %>%
    complete(week = seq.Date(min(week), max(week), by="week"), chains_assumption = c("min", "max")) %>%
    mutate(introductions_by_week_raw = replace_na(introductions_by_week_raw, 0)) %>%
    left_join(p_results_wide %>% mutate(week = as.Date(week))) %>%
    mutate(
      introductions_by_week = introductions_by_week_raw / prob_being_sampled_median,
      introductions_by_week_high = introductions_by_week_raw / prob_being_sampled_high,
      introductions_by_week_low = introductions_by_week_raw / prob_being_sampled_low
    ) %>%
    mutate(date = week)

  model_data_wide <- model_data_tmp %>%
    select(-c(
      "prob_being_sampled_median", "prob_being_sampled_high", "prob_being_sampled_low",
      "re_median",  "re_high", "re_low",
    )) %>%
    pivot_wider(
      names_from = "chains_assumption",
      values_from = c("introductions_by_week", "introductions_by_week_raw", "introductions_by_week_high", "introductions_by_week_low")
    )

  model_data <- case_data_base %>% right_join(model_data_wide, by = "date")

  model_max <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_max ~ weekly_avg_new_cases
  )
  model_max_high <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_high_max ~ weekly_avg_new_cases
  )
  model_max_low <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_low_max ~ weekly_avg_new_cases
  )
  model_min <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_min ~ weekly_avg_new_cases
  )
  model_min_high <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_high_min ~ weekly_avg_new_cases
  )
  model_min_low <- lm(
    data = model_data %>% filter(date < stop_fit_date),
    formula = introductions_by_week_low_min ~ weekly_avg_new_cases
  )
  # Reviewer requests results from model fit after re-opening as well
  model_max_fit_after <- lm(
    data = model_data %>% filter(date < stop_fit_date | (date >= re_open_date & date < end_summer_holiday_date)),
    formula = introductions_by_week_max ~ weekly_avg_new_cases
  )
  model_min_fit_after <- lm(
    data = model_data %>% filter(date < stop_fit_date | (date >= re_open_date & date < end_summer_holiday_date)),
    formula = introductions_by_week_min ~ weekly_avg_new_cases
  )

  # Thanks https://stackoverflow.com/questions/43123462/how-to-obtain-rmse-out-of-lm-result
  RMSE_max <- sqrt(crossprod(model_max$residuals) / length(model_max$residuals)) # root mean squared error
  RMSE_min <- sqrt(crossprod(model_min$residuals) / length(model_min$residuals))
  RMSE_max_fit_after <- sqrt(crossprod(model_max_fit_after$residuals) / length(model_max_fit_after$residuals))
  RMSE_min_fit_after <- sqrt(crossprod(model_min_fit_after$residuals) / length(model_min_fit_after$residuals))

  cases_coeff_max <- model_max$coefficients["weekly_avg_new_cases"]
  cases_coeff_min <- model_min$coefficients["weekly_avg_new_cases"]
  cases_coeff_max_fit_after <- model_max_fit_after$coefficients["weekly_avg_new_cases"]
  cases_coeff_min_fit_after <- model_min_fit_after$coefficients["weekly_avg_new_cases"]

  results_tmp <- data.frame(
    delay = delay,
    RMSE_max = RMSE_max,
    RMSE_min = RMSE_min,
    RMSE_max_fit_after = RMSE_max_fit_after,
    RMSE_min_fit_after = RMSE_min_fit_after,
    cases_coeff_max = cases_coeff_max,
    cases_coeff_min = cases_coeff_min,
    cases_coeff_max_fit_after = cases_coeff_max_fit_after,
    cases_coeff_min_fit_after = cases_coeff_min_fit_after
  )

  if (is_first) {
    results <- results_tmp
    is_first <- F
    best_model_data_min <- model_data
    best_model_data_max <- model_data
    best_model_max <- model_max
    best_model_max_high <- model_max_high
    best_model_max_low <- model_max_low
    best_model_min <- model_min
    best_model_min_high <- model_min_high
    best_model_min_low <- model_min_low
    best_rmse_max <- RMSE_max
    best_rmse_min <- RMSE_min
  } else {
    results <- rbind(results, results_tmp)
    if (RMSE_max < best_rmse_max) {
      best_model_data_max <- model_data
      best_model_max <- model_max
      best_model_max_high <- model_max_high
      best_model_max_low <- model_max_low
      best_rmse_max <- RMSE_max
    }
    if (RMSE_min < best_rmse_min) {
      best_model_data_min <- model_data
      best_model_min <- model_min
      best_model_min_high <- model_min_high
      best_model_min_low <- model_min_low
      best_rmse_min <- RMSE_min
    }
  }
}

# Extract best-fit delays and coefficients
cases_coeff_max <- unlist(results %>% top_n(n = 1, wt = -RMSE_max) %>% select(cases_coeff_max))
cases_coeff_min <- unlist(results %>% top_n(n = 1, wt = -RMSE_min) %>% select(cases_coeff_min))
cases_coeff_max_fit_after <- unlist(
  results %>% top_n(n = 1, wt = -RMSE_max_fit_after) %>% select(cases_coeff_max_fit_after)
)
cases_coeff_min_fit_after <- unlist(
  results %>% top_n(n = 1, wt = -RMSE_min_fit_after) %>% select(cases_coeff_min_fit_after)
)

cases_delay_max <- unlist(results %>% top_n(n = 1, wt = -RMSE_max) %>% select(delay))
cases_delay_min <- unlist(results %>% top_n(n = 1, wt = -RMSE_min) %>% select(delay))
cases_delay_max_fit_after <- unlist(results %>% top_n(n = 1, wt = -RMSE_max_fit_after) %>% select(delay))
cases_delay_min_fit_after <- unlist(results %>% top_n(n = 1, wt = -RMSE_min_fit_after) %>% select(delay))
```
Plot best-fit models and predictions
```{r}
predictions_max <- predict.lm(object = best_model_max, newdata = best_model_data_max, interval = "none")
predictions_max_high <- predict.lm(object = best_model_max_high, newdata = best_model_data_max, interval = "none")
predictions_max_low <- predict.lm(object = best_model_max_low, newdata = best_model_data_max, interval = "none")
predictions_max_all <- cbind(
  best_model_data_max,
  data.frame(fit_max = predictions_max),
  data.frame(fit_max_high = predictions_max_high),
  data.frame(fit_max_low = predictions_max_low)
)
predictions_min <- predict.lm(object = best_model_min, newdata = best_model_data_min, interval = "none")
predictions_min_high <- predict.lm(object = best_model_min_high, newdata = best_model_data_min, interval = "none")
predictions_min_low <- predict.lm(object = best_model_min_low, newdata = best_model_data_min, interval = "none")
predictions_min_all <- cbind(
  best_model_data_min,
  data.frame(fit_min = predictions_min),
  data.frame(fit_min_high = predictions_min_high),
  data.frame(fit_min_low = predictions_min_low)
)

introductions_vs_indicence_model <- ggplot() +
  geom_point(
    data = best_model_data_max %>% filter(date <= end_plot_date),
    aes(x = date, y = introductions_by_week_max, color = "max", shape = "max")
  ) +
  geom_errorbar(
    data = best_model_data_max %>% filter(date <= end_plot_date),
    aes(x = date, ymin = introductions_by_week_high_max, ymax = introductions_by_week_low_max, color = "max"),
    width = 1.5
  ) +
  geom_line(
    data = best_model_data_max %>% filter(date <= end_plot_date),
    aes(x = date, y = introductions_by_week_max, color = "max", linetype = "Phylogenetic estimate")
  ) +
  geom_point(
    data = best_model_data_min %>% filter(date <= end_plot_date),
    aes(x = date, y = introductions_by_week_min, color = "min", shape = "min")
  ) +
  geom_errorbar(
    data = best_model_data_min %>% filter(date <= end_plot_date),
    aes(x = date, ymin = introductions_by_week_high_min, ymax = introductions_by_week_low_min, color = "min"),
    width = 1.5
  ) +
  geom_line(
    data = best_model_data_min %>% filter(date <= end_plot_date),
    aes(x = date, y = introductions_by_week_min, color = "min", linetype = "Phylogenetic estimate")
  ) +
  geom_line(
    data = predictions_max_all %>% filter(date <= end_plot_date),
    aes(x = date, y = fit_max, color = "max", linetype = "Null model expectation")
  ) +
  geom_ribbon(
    data = predictions_max_all %>% filter(date <= end_plot_date),
    aes(x = date, ymin = fit_max_low, ymax = fit_max_high, fill = "max"),
    alpha = 0.3
  ) +
  geom_line(
    data = predictions_min_all %>% filter(date <= end_plot_date),
    aes(x = date, y = fit_min, color = "min", linetype = "Null model expectation")
  ) +
  geom_ribbon(
    data = predictions_min_all %>% filter(date <= end_plot_date),
    aes(x = date, ymin = fit_min_low, ymax = fit_min_high, fill = "min"),
    alpha = 0.3
  ) +
  scale_linetype_manual(
    values = c("Phylogenetic estimate" = "solid", "Null model expectation" = "dashed")
  ) + 
  scale_color_manual(
    name = "tmp",
    labels = chain_assumption_labs, 
    values = chains_assumption_colors,
    aesthetics = c("color", "fill")
  ) +
  scale_shape(name = "tmp", labels = chain_assumption_labs) +
  scale_x_date(
    date_breaks = "1 week",
    date_labels = "%b. %d",
    expand = c(0, 0)
  ) +
  shared_theme +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, vjust = 0.5),
    legend.spacing.y = unit(0, "cm"),
    legend.title = element_blank()
  ) +
  guides(fill = "none") + 
  labs(x = element_blank(), y = "Estimated total\nintroductions per week") +
  geom_rect(
    inherit.aes = F,
    data = highlight_data %>% filter(event == "Borders closed"),
    aes(xmin = date_start, xmax = date_end, ymin = 0, ymax = max(predictions_max_all$fit_max)),
    fill = "black",
    alpha = 0.3
  ) +
  scale_y_continuous(trans = "log10", expand = c(0, 0), labels = scales::label_comma())

introductions_vs_indicence_model + guides(color = guide_legend(nrow = 2), linetype = guide_legend(nrow = 2))

# # For all-europe fit
# ggsave(
#   filename = "figures/projected_introductions_all_europe.pdf", 
#   width = single_col_width,
#   height = single_col_width,
#   units = "cm"
# )
```

Write out the model variables, number of introductions averted during the lockdown

```{r}
# Note: "low" = "low Re" -> early introductions unliklier to be sampled -> more estimated total introductions
intros_averted_max <- predictions_max_all %>% 
  filter(date >= close_date, date <= re_open_date) %>%
  summarize(
    predicted_averted = sum(fit_max) - sum(introductions_by_week_max),
    predicted_percent_averted = predicted_averted / sum(fit_max),
    predicted_averted_lwr = sum(fit_max_high) - sum(introductions_by_week_low_max),
    predicted_percent_averted_lwr = predicted_averted_lwr / sum(fit_max_high),    
    predicted_averted_upr = sum(fit_max_low) - sum(introductions_by_week_high_max),
    predicted_percent_averted_upr = predicted_averted_upr / sum(fit_max_low),
  )
intros_averted_min <- predictions_min_all %>%
  filter(date >= close_date, date <= re_open_date) %>%
  summarize(
    predicted_averted = sum(fit_min, na.rm = T) - sum(introductions_by_week_min, na.rm = T),  # NA weeks had 0 est. introductions
    predicted_percent_averted = predicted_averted / sum(fit_min, na.rm = T),
    predicted_averted_lwr = sum(fit_min_high, na.rm = T) - sum(introductions_by_week_low_min, na.rm = T),
    predicted_percent_averted_lwr = predicted_averted_lwr / sum(fit_min_high, na.rm = T),    
    predicted_averted_upr = sum(fit_min_low, na.rm = T) - sum(introductions_by_week_high_min, na.rm = T),
    predicted_percent_averted_upr = predicted_averted_upr / sum(fit_min_low, na.rm = T),
  )

write.table(
  x = rbind(
    intros_averted_max %>% mutate(chains_assumption = "max"), 
    intros_averted_min %>% mutate(chains_assumption = "min")
  ),
  file = "manuscript/introductions_averted.txt",
  row.names = F
)

introsavertedmax <- unlist(intros_averted_max$predicted_averted)
introsavertedmin <- unlist(intros_averted_min$predicted_averted)

introsavertedaspermax <- unlist(intros_averted_max$predicted_percent_averted)
introsavertedaspermin <- unlist(intros_averted_min$predicted_percent_averted)

con <- file("manuscript/intoductions_by_incidence_model.tex", open = "w")
writeLines(text = paste0("\\newcommand\\bestrmsemax{", signif(best_rmse_max, digits = 2), "}"), con = con)
writeLines(text = paste0("\\newcommand\\bestrmsemin{", signif(best_rmse_min, digits = 2), "}"), con = con)
writeLines(text = paste0("\\newcommand\\casescoeffmax{", signif(cases_coeff_max, digits = 2), "}"), con = con)
writeLines(text = paste0("\\newcommand\\casescoeffmin{", signif(cases_coeff_min, digits = 2), "}"), con = con)
writeLines(text = paste0("\\newcommand\\casesdelaymax{", cases_delay_max, "}"), con = con)
writeLines(text = paste0("\\newcommand\\casesdelaymin{", cases_delay_min, "}"), con = con)
writeLines(text = paste0("\\newcommand\\introsavertedmax{", round(introsavertedmax, digits = 0), "}"), con = con)
writeLines(text = paste0("\\newcommand\\introsavertedmin{", round(introsavertedmin, digits = 0), "}"), con = con)
writeLines(text = paste0("\\newcommand\\introsavertedaspermax{", round(introsavertedaspermax * 100, digits = 2), "}"), con = con)
writeLines(text = paste0("\\newcommand\\introsavertedaspermin{", round(introsavertedaspermin * 100, digits = 2), "}"), con = con)
close(con)
```

Quantifying persistence around the lockdown: measure mean time to last sample for introductions spreading each day

```{r}
# Mean and Q1, Q3 lifespan of introductions spreading at each day in the sample period
is_first <- T
dates <- seq.Date(from = as.Date("2020-02-01"), to = as.Date("2020-12-01"), by = 1)
for (idx in seq_along(dates)) {
  start_date <- dates[idx]
  chains_on_date_data <- chain_summary_stats %>%
    filter(first_sample_date <= start_date, last_sample_date >= start_date) %>%
    mutate(days_continued_sampled = as.integer(last_sample_date - start_date)) %>% # length of continued sampling after start_date
    select(chains_assumption, days_continued_sampled) %>%
    mutate(date = start_date)
  if (is_first) {
    persistence_results <- chains_on_date_data
    is_first <- F
  } else {
    persistence_results <- rbind(persistence_results, chains_on_date_data)
  }
}

# Smooth results
is_first <- T
dates <- seq.Date(from = as.Date("2020-02-01"), to = as.Date("2020-12-01"), by = 1)
window_size <- 0 # end inclusive, so estimates are based on time from x to last_sample_date distributions summed over date - 3 days <= x <= date + 3 days
for (idx in seq_along(dates)) {
  start_date <- dates[idx]
  smooth_results <- persistence_results %>%
    filter(date <= start_date + window_size, date >= start_date - window_size) %>%
    group_by(chains_assumption) %>%
    summarize(
      n = n(),
      median = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[2],
      Q1 = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[1],
      Q3 = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[3]
    ) %>%
    mutate(date = start_date)
  if (is_first) {
    persistence_results_smooth <- smooth_results
    is_first <- F
  } else {
    persistence_results_smooth <- rbind(persistence_results_smooth, smooth_results)
  }
}

null_model <- persistence_results %>%
  filter(date <= as.Date("2020-06-15")) %>%  # corresponds to end spring in phylodynamic analysis
  group_by(chains_assumption) %>%
  summarise(
    median = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[2],
    Q1 = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[1],
    Q3 = quantile(x = days_continued_sampled, probs = c(0.25, 0.5, 0.75))[3]
  )

persisence_over_time <- ggplot(data = persistence_results_smooth %>% filter(date <= as.Date("2020-06-15")), aes(x = date, fill = chains_assumption)) +
  geom_line(aes(y = median, color = chains_assumption)) +
  geom_ribbon(aes(ymin = Q1, ymax = Q3), alpha = 0.3) +
  scale_x_date(
    date_breaks = "1 week", 
    date_labels = "%b. %d", 
    limits = c(as.Date("2020-02-24"), as.Date("2020-06-15")),
    expand = c(0, 0)
  ) +
  scale_fill_manual(labels = chain_assumption_labs, values = chains_assumption_colors, aesthetics = c("fill", "color"), name = element_blank()) +
  shared_theme +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, vjust = 0.5), 
    legend.title = element_blank()
  ) +
  scale_linetype_manual(values = c("Null model" = "dashed")) +
  labs(x = element_blank(), y = "Persistence in days") +
  geom_rect(
    inherit.aes = F, data = highlight_data %>% filter(event == "Partial lockdown"),
    aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = Inf),
    fill = "grey",
    alpha = 0.3
  ) +
  geom_hline(aes(yintercept = null_model$median[1], linetype = "Null model", color = "max")) +
  geom_hline(aes(yintercept = null_model$median[2], linetype = "Null model", color = "min"))

persisence_over_time
```

Write out the persistence of introductions at the start of lockdown, post-lockdown peak

```{r}
lockdown_start_persistence <- persistence_results_smooth %>%
  filter(date == as.Date("2020-03-17"))
lockdown_end_persistence <- persistence_results_smooth %>%
  filter(date >= as.Date("2020-04-27")) %>%
  group_by(chains_assumption) %>%
  top_n(n = 1, wt = Q1)

con <- file("manuscript/persistence.tex", open = "w")
medpersistenceatlockdownmin <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "min", "median"])
medpersistenceatlockdownmax <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "max", "median"])
writeLines(text = paste0("\\newcommand\\medpersistenceatlockdownmin{", medpersistenceatlockdownmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\medpersistenceatlockdownmax{", medpersistenceatlockdownmax, "}"), con = con)

qonepersistenceatlockdownmin <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "min", "Q1"])
qonepersistenceatlockdownmax <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "max", "Q1"])
writeLines(text = paste0("\\newcommand\\qonepersistenceatlockdownmin{", qonepersistenceatlockdownmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\qonepersistenceatlockdownmax{", qonepersistenceatlockdownmax, "}"), con = con)

qthreepersistenceatlockdownmin <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "min", "Q3"])
qthreepersistenceatlockdownmax <- unlist(lockdown_start_persistence[lockdown_start_persistence$chains_assumption == "max", "Q3"])
writeLines(text = paste0("\\newcommand\\qthreepersistenceatlockdownmin{", qthreepersistenceatlockdownmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\qthreepersistenceatlockdownmax{", qthreepersistenceatlockdownmax, "}"), con = con)

medpersistenceatpeakmin <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "min", "median"])
medpersistenceatpeakmax <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "max", "median"])
writeLines(text = paste0("\\newcommand\\medpersistenceatpeakmin{", medpersistenceatpeakmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\medpersistenceatpeakmax{", medpersistenceatpeakmax, "}"), con = con)

qonepersistenceatpeakmin <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "min", "Q1"])
qonepersistenceatpeakmax <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "max", "Q1"])
writeLines(text = paste0("\\newcommand\\qonepersistenceatpeakmin{", qonepersistenceatpeakmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\qonepersistenceatpeakmax{", qonepersistenceatpeakmax, "}"), con = con)

qthreepersistenceatpeakmin <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "min", "Q3"])
qthreepersistenceatpeakmax <- unlist(lockdown_end_persistence[lockdown_end_persistence$chains_assumption == "max", "Q3"])
writeLines(text = paste0("\\newcommand\\qthreepersistenceatpeakmin{", qthreepersistenceatpeakmin, "}"), con = con)
writeLines(text = paste0("\\newcommand\\qthreepersistenceatpeakmax{", qthreepersistenceatpeakmax, "}"), con = con)
close(con)
```

Format comparative figure

```{r}
# I have 3 legends, fill, colors, linetypes
# Extract legend to plot seperately
intervention_legend <- get_legend(
  introductions_through_time +
    guides(color = guide_legend(nrow = 2), linetype = "none") + 
    theme(legend.position = "right")
)
model_legend <- get_legend(
  introductions_vs_indicence_model +
    guides(color = "none", shape = "none", fill = "none") + 
    theme(legend.position = "right")
)
legend_col <- plot_grid(NULL, intervention_legend, model_legend, NULL, ncol = 1, rel_heights = c(1, 1, 1, 0.6), align = "v")

right_col <- plot_grid(
  persistence_through_time + theme(legend.position = "none"), 
  persisence_over_time + theme(legend.position = "none"),
  nrow = 2, align = "v", axis = "tb", labels = c("B", "D")
)
left_col <- plot_grid(
  introductions_through_time + theme(legend.position = "none"), 
  introductions_vs_indicence_model + theme(legend.position = "none"),
  nrow = 2, align = "v", axis = "tb", labels = c("A", "C")
)

top_row <- plot_grid(left_col, right_col, nrow = 1, rel_widths = c(1, 1))

plot_grid(top_row, legend_col, ncol = 2, rel_widths = c(1, 0.2))

ggsave("figures/introductions_and_persistence_v2.pdf",
  width = double_col_width * 1.5, height = single_col_width, units = "cm"
)
```
